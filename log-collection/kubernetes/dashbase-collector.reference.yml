######################## Filebeat required permissions to harvest logs ########################
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: filebeat
  labels:
    k8s-app: filebeat
rules:
- apiGroups: [""] # "" indicates the core API group
  resources:
  - namespaces
  - pods
  verbs:
  - get
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  # Change namespace to the ns where your Filebeat daemonset located(which is defined at the last scope of this file)
  namespace: dashbase
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  # Change namespace to the ns where your Filebeat daemonset located(which is defined at the last scope of this file)
  namespace: dashbase
  labels:
    k8s-app: filebeat
---
######################## Filebeat required configuration to harvest logs ########################
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  # Change namespace to the ns where your Filebeat daemonset located(which is defined at the last scope of this file)
  namespace: dashbase
  labels:
    k8s-app: filebeat
data:
  filebeat.yml: |-
    filebeat.autodiscover:
      providers:
        - type: kubernetes
          # ############################################################################################################
          # Filebeat will add the below annotations into events if they're not absent.
          # Dashbase depends on these annotations to figure out how to parse the event.
          # To add a message like:
          # "_message_parser": {
          #    "type": "log4j",
          #    "pattern": "%d %m%n"
          #   }
          # You need to add these annotations to pods:
          # 1. dashbase.io/filebeat.parser.type: log4j
          # 2. dashbase.io/filebeat.parser.pattern: "%d %m%n"
          # (It's working with below rename processor.)
          # See more information here:
          # https://dashbase.atlassian.net/wiki/spaces/DK/pages/6816075/Parser+Reference#ParserReference-Filebeat
          # ############################################################################################################
          include_annotations:
            - "dashbase.io/filebeat.parser.type"
            - "dashbase.io/filebeat.parser.pattern"
            - "dashbase.io/filebeat.parser.defaultTimeZone"
            - "dashbase.io/filebeat.parser.parsers.0.type"
            - "dashbase.io/filebeat.parser.parsers.0.pattern"
            - "dashbase.io/filebeat.parser.parsers.0.defaultTimeZone"
            - "dashbase.io/filebeat.parser.parsers.1.type"
            - "dashbase.io/filebeat.parser.parsers.1.pattern"
            - "dashbase.io/filebeat.parser.parsers.1.defaultTimeZone"
            - "dashbase.io/filebeat.parser.parsers.2.type"
            - "dashbase.io/filebeat.parser.parsers.2.pattern"
            - "dashbase.io/filebeat.parser.parsers.2.defaultTimeZone"

          templates:
            - config:
                - type: docker
                  close_removed: false
                  containers.ids:
                    - "${data.kubernetes.container.id}"
                  # ####################################################################################################
                  # By default, Filebeat will harvest logs line-by-line.
                  # If one pod had annotated with `dashbase.io/filebeat.multiline.pattern`, `dashbase.io/filebeat.multiline.negate`, `dashbase.io/filebeat.multiline.match`,
                  # Filebeat will these multiline configurations when harvesting.
                  # See the more information and multiline rules here:
                  # https://www.elastic.co/guide/en/beats/filebeat/6.6/multiline-examples.html#multiline-examples
                  # ####################################################################################################
                  multiline:
                    pattern: ${data.kubernetes.annotations.dashbase.io/filebeat.multiline.pattern:^.}
                    negate: ${data.kubernetes.annotations.dashbase.io/filebeat.multiline.negate:true}
                    match: ${data.kubernetes.annotations.dashbase.io/filebeat.multiline.match:after}

    processors:
      - add_cloud_metadata:
      # ################################################################################################################
      # This rename processor is working with the above include_annotations to pass message parser to dashbase.
      # ################################################################################################################
      - rename:
          fields:
            - from: "kubernetes.annotations.dashbase.io/filebeat.parser"
              to: "fields._message_parser"
          ignore_missing: true
          fail_on_error: false

    # ##################################################################################################################
    # Scraped by telegraf.
    # This enable HTTP endpoint for Filebeat, so that we can see its metrics.
    # Ref: https://www.elastic.co/guide/en/beats/filebeat/6.6/http-endpoint.html
    # ##################################################################################################################
    http.enabled: true
    http.host: 127.0.0.1
    http.port: 1050

    # ##############################################
    # Set your elsticsearch host and protocol here.
    # ##############################################
    output.elasticsearch:
      hosts: ['http://proxy:9200']
      index: "filebeat"  # change it to your dashbase table name
      ssl.verification_mode: "none"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: telegraf-config
  # Change namespace to the ns where your Filebeat daemonset located(which is defined at the last scope of this file)
  namespace: dashbase
  labels:
    k8s-app: telegraf
data:
  telegraf.conf: |-
    [global_tags]
        table = "filebeat"  # change the table name to your dashbase table name
    [agent]
        interval = "10s"
        debug = false
        round_interval = true
        flush_interval = "10s"
        flush_jitter = "0s"
        logfile = "/var/log/telegraf/telegraf.log"
        omit_hostname = false

    ###############################################################################
    #                                  OUTPUTS                                    #
    ###############################################################################

    [[outputs.prometheus_client]]
        listen = ":29273"
        expiration_interval = "60s"

    ###############################################################################
    #                                  INPUTS                                     #
    ###############################################################################

    # ##################################################################################################################
    # Optional night-watch commands
    # Notes: It will not work well when you're scrape specified pods on host or have custom multiline configration.
    #
    [[inputs.exec]]
        commands = ["night-watch watch-append -p '/var/lib/docker/containers/*/*.log'"]
        timeout = "10s"
        data_format = "influx"
    # ##################################################################################################################

    # scrape Filebeat metrics
    [[inputs.http]]
        urls = ["http://localhost:1050/stats"]
        data_format = "json"
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  # Change namespace to whatever you want
  namespace: dashbase
  labels:
    app: dashbase-daemonsets
    component: filebeat
spec:
  selector:
    matchLabels:
      app: dashbase-daemonsets
      component: filebeat
  template:
    metadata:
      labels:
        app: dashbase-daemonsets
        component: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat-oss:6.8.5
        args: [
          "-c", "/etc/filebeat.yml",
          "-e",
        ]
        securityContext:
          runAsUser: 0
        # Increase the cpu and memory if you have high throughput requirements
        resources:
          limits:
            cpu: 1
            memory: 1Gi
          requests:
            cpu: 1
            memory: 1Gi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      - name: telegraf
        image: dashbase/night-watch:v1.1.0
        args: [
          "--config", "/etc/telegraf.conf"
        ]
        resources:
          limits:
            cpu: 200m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 500Mi
        env:
          # The PushGateway where Telegraf push metrics to.
          # It will push metrics once per minute.
          - name: GATEWAY_URL
            value: "http://pushgateway:9091/metrics/job/filebeat/instance/$HOSTNAME"
        volumeMounts:
        - name: telegraf-config
          mountPath: /etc/telegraf.conf
          readOnly: true
          subPath: telegraf.conf
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0600
          name: filebeat-config
      - name: telegraf-config
        configMap:
          defaultMode: 0600
          name: telegraf-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate
